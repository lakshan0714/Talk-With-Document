{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embeddings import Embedding\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import traceback\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "\n",
    "    def __init__ (self, model_name = \"BAAI/bge-large-en-v1.5\",\n",
    "            model_kwargs = {\"device\": \"cpu\"},\n",
    "            encode_kwargs = {\"normalize_embeddings\": True},\n",
    "            collection_name=\"Vector_DB\",\n",
    "            llm_model=\"mistral-large-latest\"        \n",
    "    ):\n",
    "    \n",
    "        \n",
    "        self.model_name=model_name\n",
    "        self.model_kwargs=model_kwargs\n",
    "        self.encode_kwargs=encode_kwargs\n",
    "        self.collection_name = collection_name\n",
    "        self.llm_model=llm_model\n",
    "\n",
    "       \n",
    "       \n",
    "     \n",
    "\n",
    "\n",
    "        self.embeddings = HuggingFaceBgeEmbeddings(\n",
    "            model_name=self.model_name,\n",
    "            model_kwargs=self.model_kwargs,\n",
    "            encode_kwargs=self.encode_kwargs)\n",
    "    \n",
    "\n",
    "        self.new_vector_store = FAISS.load_local(\n",
    "                           \"faiss_local\" , self.embeddings, allow_dangerous_deserialization=True)\n",
    "        \n",
    "        self.retriever=self.new_vector_store.as_retriever()\n",
    "\n",
    "        \n",
    "        self.llm=ChatMistralAI(\n",
    "            model=self.llm_model,\n",
    "            api_key=\"j3Equsw7oun87HjYroXXnFJ9LvWcUoDE\"\n",
    "\n",
    "            #temperature=0.2\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Provide page no of the pdf related to answer. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "        self.prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", self.system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "        self.question_answer_chain = create_stuff_documents_chain(self.llm, self.prompt)\n",
    "        self.chain = create_retrieval_chain(self.retriever, self.question_answer_chain)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def get_response(self,query):\n",
    "        try:\n",
    "            response=self.chain.invoke({\"input\":query})\n",
    "            return response\n",
    "        \n",
    "        except Exception as e:\n",
    "\n",
    "            traceback.print_exc()\n",
    "            return f\"Couldn't process the response: {e}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LAKSHAN\\AppData\\Local\\Temp\\ipykernel_18412\\4077312324.py\", line 62, in get_response\n",
      "    response=self.chain.invoke({\"input\":query})\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5354, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3022, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py\", line 494, in invoke\n",
      "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1927, in _call_with_config\n",
      "    context.run(\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\config.py\", line 396, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py\", line 481, in _invoke\n",
      "    **self.mapper.invoke(\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3727, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3727, in <dictcomp>\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3711, in _invoke_step\n",
      "    return context.run(\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5354, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3024, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\retrievers.py\", line 254, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\retrievers.py\", line 247, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\vectorstores\\base.py\", line 1080, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 641, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 514, in similarity_search_with_score\n",
      "    docs = self.similarity_search_with_score_by_vector(\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 415, in similarity_search_with_score_by_vector\n",
      "    scores, indices = self.index.search(vector, k if filter is None else fetch_k)\n",
      "  File \"c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\faiss\\class_wrappers.py\", line 329, in replacement_search\n",
      "    assert d == self.d\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "chat=Chatbot()\n",
    "x=chat.get_response(query=\"How to adjust the temprature?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j3Equsw7oun87HjYroXXnFJ9LvWcUoDE'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"]=os.getenv(\"API_URL\")\n",
    "os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\LAKSHAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector_store = FAISS.load_local(\n",
    "                           \"faiss_local\" , embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=new_vector_store.as_retriever()\n",
    "\n",
    "        \n",
    "llm=ChatMistralAI(\n",
    "            model=\"mistral-large-latest\",\n",
    "            api_key=\"j3Equsw7oun87HjYroXXnFJ9LvWcUoDE\"\n",
    "            #temperature=0.2\n",
    "            \n",
    "        )\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "   \n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever,question_answer_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vector Database Created Succesfully'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path=\"4249011.pdf\"\n",
    "e=Embedding()\n",
    "e.Create_Embeddings(PDF_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=chain.invoke({\"input\":\"How to adjust the temprature?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To adjust the temperature:\n",
      "\n",
      "1. **Press SET for 1 second**: The current temperature value will start flashing.\n",
      "2. **Use UP or DOWN**: Increase or decrease the value to your desired temperature.\n",
      "3. **Press SET again**: This will confirm and save the new temperature value.\n",
      "\n",
      "The initial setpoints are +4°C for the refrigerator and -18°C for the freezer. Adjust only if necessary.\n"
     ]
    }
   ],
   "source": [
    "print(x['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
